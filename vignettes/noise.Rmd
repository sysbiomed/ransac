---
title: "RANSAC Noise"
author: "André Veríssimo - IDMEC/IST - SELS"
output: 
  html_document:
    toc: true
    dev: svg
    self_contained: true
    number_sections: true
params:
  mc.cores: !r 15
  # seed to make reproducible results
  my.seed: !r 1991
  
  # Number of lambdas to test
  nlambda: !r 1000
  # Minimum ratio of lambda from initial
  lambda.min.ratio: !r 1e-5
  
  # training / test percentage
  train.perc: !r 1

  #
  perturbation.perct: !r seq(0, .5, 0.01)

  #
  # Ransac
 
  # use pre-set models -- only need to change k and my.family
  pre.set: '1000vars-sparse'
  
  # number of iterations
  k: !r 2000
  
  # type of model to use
  my.family: 'binomial.glmnet.auc'
  
  # Number of observations in data
  obs: !r 100
  
  # True model
  base.model: !r c(0,2,-3,1.5)
  
  # minimum number of points to use
  n: !r 30
  # threshold to identify inliers
  threshold: !r .7 # old: 0.04
  # number of inliers to consider good model
  good.fit.perct: !r .1
  # chose one of the lambda / alpha / penalty
  alpha.baseline: !r .5
  
  #
  # Show arguments
  
  only.show.mod: !r .05
---

```{r setup,include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries,include=FALSE}
## Libraries required

# output nothing
suppressPackageStartupMessages({
  library(devtools)
  source(file.path('..', 'R-aux', 'dataset_brca_tnbc.R'))
  #library(ransac)
  devtools::load_all('../')
  library(brca.data)
  library(futile.logger)
  library(glmnet)
  library(verissimo)
  library(dplyr)
  library(ggbeeswarm)
  library(ggplot2)
  library(reshape2)
  library(digest)
  library(gen.rna.seq)
})
```

```{r logging.settings,include=FALSE}
# Setup logging to write to file `logger.txt` and using DEBUG level

#flog.layout(layout.simple)
layout <- layout.format('~m')
flog.layout(layout)
flog.appender(appender.tee('logger.txt'))
flog.threshold(DEBUG)
```

# Parameters

```{r params,include=FALSE}
#if (exists('my.params')) {
#  params <- my.params
#}

# create variables with params names
for (ix.name in names(params)) { 
  assign(ix.name, params[[ix.name]])
}

# generated parameters
family.fun <- ransac.family(my.family)
```

```{r, include=FALSE}
## Define synthetic data steps
if (pre.set == '3vars') {
  # True model
  base.model         <- c(0,2,-3,1.5)
} else if (pre.set == '10vars-sparse') {
  set.seed(my.seed)
  base.model         <- c(0,runif(10, min = -10, max = 10))
  base.model[sample(seq(length(base.model)), 10 - 2)] <- 0 # keep only 2 variables
} else if (pre.set == '1000vars-sparse') {
  set.seed(my.seed)
  base.model         <- c(0,runif(1000, min = -10, max = 10))
  base.model[sample(seq(length(base.model)), 1000 - 80)] <- 0 # keep only 80 variables
}

#
penalty.factor <- array(1, length(base.model) - 1)
sort.by        <- 'logit_class'
ransac.tbl     <- data.frame()
baseline.tbl   <- data.frame()
```


```{r, include=FALSE}

#
#
print.outlier.table <- function(my.model, xdata.train, ydata.train, lambda.baseline) {
  
  miscl.all.ix <- ydata.train$outliers %in% c('perturbation', 'natural')
  flog.info('')
  flog.info('Trained on the perturbed dataset and tested against it.')
  miscl.pert.prob <- family.fun$predict(my.model, newx = xdata.train, lambda = lambda.baseline)

  miscl.pert <- as.numeric(levels(ydata.train$logit_class)[ydata.train$logit_class]) - round(miscl.pert.prob)
  flog.info('')
  flog.info('```{r}')
  flog.info('Perturbed model:\n    % 6d false p.\n  + % 6d false n.\n  -----------------\n    % 6d / %d',
            sum(miscl.pert > 0), sum(miscl.pert < 0), sum(miscl.pert != 0), length(miscl.pert))
  flog.info('')
  flog.info('```')
  my.data$ydata$outliers[miscl.pert != 0]
  miscl.pert.diff <- ydata.train$prob - abs(round(miscl.pert.prob))
  flog.info('')
  table.ix <- miscl.pert != 0 | miscl.all.ix
  table.aux <- cbind(id = rownames(ydata.train)[table.ix],
                     ydata.train[table.ix,], 
                     predicted_prob = miscl.pert.prob[table.ix],
                     pred_class = round(miscl.pert.prob[table.ix]),
                     diff_class = miscl.pert[table.ix])

  table.aux <- arrange_(table.aux, 'outliers', 'diff_class', 'id')
  flog.info('')
  if (nrow(table.aux) > 0) {
    flog.info('Notes on significance of predicted, as it is calculated by logit_class(i) - round(pred(i))')
    flog.info('')
    flog.info(' - -1: false positive')
    flog.info(' -  0: correct classification')
    flog.info(' - +1: false negative')
    flog.info('')
    flog.info('Table with all outliers in model (plus all perturbed/natural)')
    flog.info('')
    cat(knitr::kable(table.aux), sep = '\n')
  }
  # not showing the distribution of error
  #flog.info('')
  #flog.info('')
  #print(ggplot(melt(miscl.pert.diff)) + theme_minimal() +
  #  geom_freqpoly(aes(value), color = '#E69F00', binwidth = 0.05) + 
  #  ggtitle('Distribution of error to real probabilities'))
  flog.info('')
  
  return(data.frame(perct = perct,
                    inliers = sum(abs(table.aux$diff_class[table.aux$outliers == 'inlier'])),
                    total.inliers = sum(ydata.train$outliers == 'inlier'),
                    natural = sum(abs(table.aux$diff_class[table.aux$outliers == 'natural'])),
                    total.natural = sum(ydata.train$outliers == 'natural'),
                    perturbation = sum(abs(table.aux$diff_class[table.aux$outliers == 'perturbation'])),
                    total.perturbation = sum(ydata.train$outliers == 'perturbation')))
}
```

```{r show.params,echo=FALSE}
{
  flog.info('Parameters:')
  flog.info('')
  flog.info('---- General --------------------')
  flog.info('                           Family: %s', my.family)
  flog.info('         initial seed for reprod.: %d', my.seed)
  flog.info('                       # of cores: %d', mc.cores)
  flog.info('             # of lambdas to test: %d', nlambda)
  flog.info('                 lambda.min.ratio: %g', lambda.min.ratio)
  flog.info('----------------------------------')
  flog.info('')
  flog.info('---- True Model ------------------')
  flog.info('                    pre.set model: %s', pre.set)
  flog.info('                    vars in model: %d', length(base.model) - 1)
  flog.info('           non-zero vars in model: %d', sum(base.model[2:length(base.model)] != 0))
  flog.info('                        intercept: %g', base.model[1])
  flog.info('----------------------------------')
  flog.info('')
  flog.info('---- RANSAC ----------------------')
  flog.info('                      Train set %%: %g', train.perc)
  flog.info('                     k iterations: %d', k)
  flog.info('          minimum number of cases: %d', n)
  flog.info('      threshold to keep (squared): %g', threshold)
  flog.info('   %% of inliers to consider model: %g', good.fit.perct)
  flog.info('       alpha to be used in RANSAC: %g', alpha.baseline)
}
```

```{r, eval=FALSE, include=FALSE}
alpha.baseline <- 1
threshold <- .9
k <- 500
#
set.seed(my.seed)
perct <- .1
my.data <- gen.synth(obs = obs, 
                     new.coef = base.model, 
                     perturbation.perct = perct, 
                     xdata.fun = gen.rna.seq::gen.rna.seq,
                     perfect.model = TRUE,
                     perturbation.random = TRUE)
my.data$ydata
#
xdata.train <- my.data$xdata

# normalize xdata
xdata.train.mean <- sapply(seq(ncol(xdata.train)), function(ix) { mean(xdata.train[,ix])})
xdata.train.sd   <- sapply(seq(ncol(xdata.train)), function(ix) { sd(xdata.train[,ix])})
xdata.train <- sapply(seq(ncol(xdata.train)), function(ix.col) { (xdata.train[,ix.col] - xdata.train.mean[ix.col]) / xdata.train.sd[ix.col]})

ydata.train <- as.data.frame(my.data$ydata)
ydata.train$logit_class <- as.factor(ydata.train$logit_class)
ydata.train$real_class  <- as.factor(ydata.train$real_class)
  
colnames(ydata.train) <- colnames(my.data$ydata)

cv.folds <- balanced.cv.folds(which(ydata.train$logit_class == 0), which(ydata.train$logit_class == 1))
foldid   <- array(0, nrow(ydata.train))
foldid[ydata.train$logit_class == 0] <- cv.folds$output[[1]]
foldid[ydata.train$logit_class == 1] <- cv.folds$output[[2]]

baseline <- cv.glmnet(x                = xdata.train, 
                      y                = ydata.train$logit_class,
                      family           = 'binomial', 
                      alpha            = alpha.baseline,
                      nlambda          = nlambda,
                      lambda.min.ratio = lambda.min.ratio,
                      #
                      standardize    = F,
                      intercept      = F,
                      nfolds         = 10,
                      foldid         = foldid,
                      penalty.factor = penalty.factor,
                      mc.cores       = 1)

  
#
lambda.baseline <- baseline$lambda.min

# perturbed model
suppressWarnings({
  baseline.model.pert <- family.fun$fit.model(my.data$xdata, my.data$ydata$logit_class,
                                              lambda.baseline, penalty.factor = penalty.factor,
                                              alpha          = alpha.baseline)
})

#
baseline.coef <- coef(baseline.model.pert, s = lambda.baseline)[,1]

ransac.cache <- file.path('..', 
                            'cache', 
                            sprintf('noise-perct_%.3f-k_%d-n_%d-f_%s-t_%.4f-g_%.4f-a_%.3f-p_%s-x_%s-y_%s.RData',
                                    perct,
                                    k, 
                                    n, 
                                    my.family,
                                    threshold,
                                    good.fit.perct,
                                    alpha.baseline,
                                    strtrim(digest(penalty.factor, algo = 'md5'), 10),
                                    strtrim(digest(xdata.train, algo = 'md5'), 10),
                                    strtrim(digest(ydata.train, algo = 'md5'), 10)))
  if (file.exists(ransac.cache)){
    load(ransac.cache)
  } else {
    result.ransac <- ransac::ransac(xdata  = xdata.train, 
                                    ydata  = as.numeric(levels(ydata.train$logit_class)[ydata.train$logit_class]), 
                                    k      = k, 
                                    n      = n, 
                                    family = my.family, 
                                    #
                                    threshold      = threshold, 
                                    good.fit.perct = good.fit.perct, 
                                    #
                                    mc.cores       = mc.cores,
                                    lambda         = lambda.baseline,
                                    alpha          = alpha.baseline,
                                    intercept      = F,
                                    penalty.factor = penalty.factor)
    save(result.ransac, file = ransac.cache, compress = 'bzip2')
  }

ransac.coef <- coef(result.ransac$models$all.inliers.consensus, s = lambda.baseline)[,1]

aa <- data.frame(prediction = family.fun$predict(baseline.model.pert, newx = xdata.train, lambda = lambda.baseline)[,1],
                 observation = as.numeric(levels(ydata.train$logit_class))[ydata.train$logit_class],
                 prob = ydata.train$prob,
                 outliers = ydata.train$outliers,
                 type = array('Baseline', nrow(ydata.train)),
                 y.id = as.numeric(rownames(ydata.train)))


bb <- data.frame(prediction = family.fun$predict(result.ransac$models$all.inliers.consensus, newx = xdata.train, lambda = lambda.baseline)[,1],
                 observation = as.numeric(levels(ydata.train$logit_class))[ydata.train$logit_class],
                 prob = ydata.train$prob,
                 outliers = ydata.train$outliers,
                 type = array(sprintf('RANSAC (%d / %d)', result.ransac$models$all.inliers.consensus$nobs, nrow(ydata.train)), 
                              nrow(ydata.train)),
                 y.id = as.numeric(rownames(ydata.train)))

miscl.ix <- unique(c(aa$y.id[(aa$prediction > 0.5) * 1 - aa$observation != 0], 
                     bb$y.id[(bb$prediction > 0.5) * 1 - bb$observation != 0]),
                     as.numeric(rownames(ydata.train[ydata.train$outliers != 'inlier',])))
aa <- aa[miscl.ix,]
bb <- bb[miscl.ix,]

my.out.order <- c('observation', 'prob', 'prediction')
bb      <- arrange_(bb, .dots = my.out.order)
aa      <- arrange_(aa, .dots = my.out.order)
aa$id <- seq(nrow(aa))
bb$id   <- seq(nrow(bb))

out.tbl <- rbind(aa, bb)
out.tbl$is.miscl <- factor((out.tbl$prediction > 0.5) * 1 - out.tbl$observation == 0, levels = c(TRUE, FALSE), labels = c('','miscl.'))
out.melt <- melt(out.tbl, id.vars = c('id', 'y.id', 'type', 'is.miscl'), measure.vars = c('prediction', 'prob'))

t(rbind(base.model,ransac.coef, baseline.coef))[base.model != 0,]
ggplot(out.melt) + theme_minimal() + 
  labs(title = 'Showing only misclassifications and perturbations', subtitle = sprintf('alpha = %.4f | lambda = %.4f | threshold = %.3f | k = %d', alpha.baseline, lambda.baseline, threshold, k)) +
  geom_point(aes(y = value, x = id, color = variable, shape = is.miscl)) +
  geom_vline(aes(xintercept = sum(ydata.train$logit_class[miscl.ix] == 0) + .5), color = 'black', linetype = 'dotted') +
  facet_wrap( ~ type, ncol = 1)

#print.outlier.table(baseline.model.pert, xdata.train, ydata.train, lambda.baseline)
```


# Generate data and ransac vs. baseline

```{r,results='asis',echo=FALSE}
#for (my.seed in 1985:2017) {
new.ydata <- data.frame()
for (perct in perturbation.perct) {
  set.seed(my.seed)
  
  my.data <- gen.synth(obs = obs, 
                       new.coef = base.model, 
                       perturbation.perct = perct, 
                       xdata.fun = gen.rna.seq::gen.rna.seq,
                       perfect.model = TRUE,
                       perturbation.random = TRUE)
  #
  #
  sorted.ix <- sort(my.data$ydata[[sort.by]], index.return = T)$ix
  my.data$xdata <- my.data$xdata[sorted.ix,]
  my.data$ydata <- my.data$ydata[sorted.ix,]
  #
  my.data$ydata$logit_class <- as.factor(my.data$ydata$logit_class)
  my.data$ydata$real_class  <- as.factor(my.data$ydata$real_class)
  #
  class.0.count <- sum(my.data$ydata[[sort.by]] == 0)
  class.1.count <- sum(my.data$ydata[[sort.by]] == 1)
  
  # Build big table
  new.ydata <- rbind(new.ydata, 
                     cbind(my.data$ydata, 
                           pert = perct, 
                           count.0 = class.0.count + .5, 
                           count.a = class.0.count / 2, 
                           count.b = class.0.count * 1.5, 
                           x = seq(nrow(my.data$ydata))))
  #
  #  
  # GLMNET 
  
  # Estimate lambda
  
  sets.ix <- balanced.train.and.test(which(my.data$ydata$logit_class == 1), 
                                     which(my.data$ydata$logit_class == 0), 
                                     train.perc = 1, join.all = T)
  train.ix <- sets.ix$train
  test.ix  <- sets.ix$test
  
  #  
  xdata.train <- my.data$xdata[train.ix, ]
  # normalize xdata
  xdata.train.mean <- sapply(seq(ncol(xdata.train)), function(ix) { mean(xdata.train[,ix])})
  xdata.train.sd   <- sapply(seq(ncol(xdata.train)), function(ix) { sd(xdata.train[,ix])})
  #xdata.train <- xdata.train / xdata.train.sd
  xdata.train <- sapply(seq(ncol(xdata.train)), function(ix.col) { (xdata.train[,ix.col] - xdata.train.mean[ix.col]) / xdata.train.sd[ix.col]})
  
  ydata.train <- as.data.frame(my.data$ydata[train.ix,])
  colnames(ydata.train) <- colnames(my.data$ydata)
  
  if (grepl('GLMNET', family.fun$model.name)) {
    cv.folds <- balanced.cv.folds(which(ydata.train$logit_class == 0), which(ydata.train$logit_class == 1))
    foldid   <- array(0, nrow(ydata.train))
    foldid[ydata.train$logit_class == 0] <- cv.folds$output[[1]]
    foldid[ydata.train$logit_class == 1] <- cv.folds$output[[2]]
    #   
    baseline <- cv.glmnet(xdata.train, 
                          as.numeric(levels(ydata.train$logit_class))[ydata.train$logit_class], 
                          family = 'binomial', 
                          alpha            = alpha.baseline,
                          nlambda          = 100, 
                          lambda.min.ratio = lambda.min.ratio,
                          #
                          standardize    = F,
                          intercept      = F,
                          nfolds         = 10,
                          foldid         = foldid,
                          penalty.factor = penalty.factor,
                          mc.cores       = 1)
    lambda.baseline <- baseline$lambda.min
  } else {
    lambda.baseline <- 0
    alpha.baseline <- 0
  }
  #
  
  # Create baseline models
  
  # perturbed model
  suppressWarnings({
  baseline.model.pert <- family.fun$fit.model(xdata.train, ydata.train$logit_class,
                                              lambda.baseline, penalty.factor = penalty.factor,
                                              alpha          = alpha.baseline)
  })
  # Ransac

  ransac.cache <- file.path('..', 
                            'cache', 
                            sprintf('noise-perct_%.3f-k_%d-n_%d-f_%s-t_%.4f-g_%.4f-a_%.3f-p_%s-x_%s-y_%s.RData',
                                    perct,
                                    k, 
                                    n, 
                                    my.family,
                                    threshold,
                                    good.fit.perct,
                                    alpha.baseline,
                                    strtrim(digest(penalty.factor, algo = 'md5'), 10),
                                    strtrim(digest(xdata.train, algo = 'md5'), 10),
                                    strtrim(digest(ydata.train, algo = 'md5'), 10)))
  if (file.exists(ransac.cache)){
    load(ransac.cache)
  } else {
    do.nothing <- capture.output({
      result.ransac <- ransac::ransac(xdata  = xdata.train, 
                                      ydata  = as.numeric(levels(ydata.train$logit_class)[ydata.train$logit_class]), 
                                      k      = k, 
                                      n      = n, 
                                      family = my.family, 
                                      #
                                      threshold      = threshold, 
                                      good.fit.perct = good.fit.perct, 
                                      #
                                      mc.cores       = mc.cores,
                                      lambda         = lambda.baseline,
                                      alpha          = alpha.baseline,
                                      intercept      = F,
                                      penalty.factor = penalty.factor)
    })
    save(result.ransac, lambda.baseline, file = ransac.cache, compress = 'bzip2')
  }
  
  #
  #
  baseline.output <- capture.output({
    baseline.tbl.newline <- print.outlier.table(baseline.model.pert, xdata.train, ydata.train, lambda.baseline)
  })
  
  baseline.tbl <- rbind(baseline.tbl, baseline.tbl.newline)
  
  #
  #
  has.ransac.warning <- FALSE
  if (length(result.ransac$models$all.inliers.consensus) < 1 || is.na(result.ransac$models$all.inliers.consensus)) {
    ransac.tbl.newline <- data.frame(perct = perct,
                                   inliers = NA,
                                   total.inliers = sum(ydata.train$outliers == 'inlier'),
                                   natural = NA,
                                   total.natural = sum(ydata.train$outliers == 'natural'),
                                   perturbation = NA,
                                   total.perturbation = sum(ydata.train$outliers == 'perturbation'))
    ransac.tbl <- rbind(ransac.tbl, ransac.tbl.newline)
    has.ransac.warning <- TRUE
  } else {
    ransac.output <- capture.output({
      ransac.tbl.newline <- print.outlier.table(result.ransac$models$all.inliers.consensus, xdata.train, ydata.train, lambda.baseline)
    })
    
    best.ransac.ix <- result.ransac$best.ix$all.inliers.consensus
    consensus.inliers <- result.ransac$error.array[[best.ransac.ix]]$inliers.ix
    ransac.tbl.newline$model.inlier       <- sum(ydata.train[consensus.inliers, ]$outliers == 'inlier')
    ransac.tbl.newline$model.natural      <- sum(ydata.train[consensus.inliers, ]$outliers == 'natural')
    ransac.tbl.newline$model.perturbation <- sum(ydata.train[consensus.inliers, ]$outliers == 'perturbation')

    ransac.tbl <- rbind(ransac.tbl,ransac.tbl.newline)
  }

  #
  #
  # Output
  # 
  
  if (only.show.mod == FALSE || (100 * perct) %% (100 * only.show.mod) == 0) {
    flog.info('\n## Testing for noise %% at %.3f', perct)
    #
    flog.info('\n### Data')
    flog.info('')
    flog.info('    Counts:')
    flog.info('      Class 0: %d', class.0.count)
    flog.info('      Class 1: %d', class.1.count)
    #
    flog.info('')
    
    print(ggplot(my.data$ydata) + 
      #
      theme_minimal() + 
      labs(x = '', y = 'Class Probability', shape = "Class", color = "Oulier") + 
      ggtitle('') + 
      # Class distinction
      geom_vline(aes(xintercept = class.0.count + .5), color = 'black', linetype = 'dotted') +
      geom_text(aes(x=class.0.count / 2), y = 1.05, label= "Class 0") +
      geom_text(aes(x=class.0.count + class.1.count / 2), y = 1.05, label= "Class 1") +
      #
      scale_shape_manual(values=c(1,9)) +
      scale_color_manual(values=c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")) +
      scale_y_continuous(minor_breaks = waiver(), breaks = c(0,0.25, 0.5, .75, 1), expand = c(0.05,0.05)) + 
      scale_x_discrete(breaks = NULL) +
      #
      geom_point(aes(x = seq(nrow(my.data$ydata)), y = prob, color = outliers, shape = logit_class)))
    #
    flog.info('')
    # 
    
  
    if (grepl('GLMNET', family.fun$model.name)) {
      flog.info('')
      flog.info('    Running GLMNET with cross-validation to find lambda parameter')
      flog.info('      - Found lambda = %g to use in RANSAC if necessary', lambda.baseline)
    }
  
    flog.info('')
    flog.info('')
    flog.info('### RANSAC & Baseline')
    flog.info('')

    suppressWarnings({
      plot.info <- plot(result.ransac, xdata = xdata.train, 
           ydata          = as.numeric(levels(ydata.train$real_class)[ydata.train$real_class]),
           baseline       = baseline.model.pert,
           family         = my.family, 
           name           = sprintf('Perct: %.3f (distance to class)', perct),
           lambda         = lambda.baseline,
           alpha          = alpha,
           only_consensus = T,
           show.misclass = F)
    })
  
    flog.info('\n```{r}')
    cat( plot.info$debug, sep = '\n')
    flog.info('```')
    
    #
    #
    flog.info('')
    flog.info('### Misclassification Tables')
    flog.info('')
    flog.info('#### Baseline')
  
    cat(baseline.output, sep = '\n')
  
    flog.info('')
    flog.info('#### RANSAC Consensus')
  
    if (has.ransac.warning) {
      flog.info('')
      flog.info('```{r}')
      flog.info('WARNING: RANSAC consensus does not have any good fit with %.3f of perturbation', perct)
      flog.info('```')
    } else {
      cat(ransac.output, sep = '\n')
    }  
  
    
  }
}
#} for to test seed
```

# Summary of noise experiments

```{r, echo=FALSE}
baseline.tbl.sum <- data.frame(perct = baseline.tbl$total.perturbation,
                               inliers = baseline.tbl$inliers / baseline.tbl$total.inliers,
                               natural = baseline.tbl$natural / baseline.tbl$total.natural,
                               perturbation = baseline.tbl$perturbation / baseline.tbl$total.perturbation,
                               type = array('Baseline', nrow(baseline.tbl)))

# 
ransac.tbl.sum <- data.frame(perct = baseline.tbl$total.perturbation,
                             inliers = ransac.tbl$inliers / ransac.tbl$total.inliers,
                             natural = ransac.tbl$natural / ransac.tbl$total.natural,
                             perturbation = ransac.tbl$perturbation / ransac.tbl$total.perturbation,
                             type = array('RANSAC', nrow(ransac.tbl)))

tbl.melt <- melt(rbind(baseline.tbl.sum, ransac.tbl.sum), id.vars = c('perct', 'type'))
tbl.melt$identifier <- paste0(tbl.melt$type, ' ', tbl.melt$variable)
tbl.melt <- subset(tbl.melt, !is.nan(value))
#
ggplot(tbl.melt, aes(x = perct, y = value, color = identifier, shape = identifier, linetype = identifier)) + 
  theme_minimal() + theme(legend.position = "top") + expand_limits(y = c(0,1)) +
  #  facet_wrap( ~ type, ncol = 1) +
  geom_line() + geom_point() + 
    scale_color_manual(name = '', values = c('#9E5D00', '#007700', '#D55E00', '#009E73')) + 
  scale_shape_manual(name = '', values = c(1, 1, 5, 5)) + 
  scale_linetype_manual(name = '', values = c('solid', 'solid', 'longdash', 'longdash')) +
  #geom_line(data = tbl.melt.ransac) + geom_point(data = tbl.melt.ransac) + 
  labs(title = 'Misclassification rate by type of data point')
```

# Summary of Outliers in RANSAC models

```{r, echo=FALSE}
ransac.tbl.outlier <- data.frame(perct = ransac.tbl$total.perturbation,
                                 inliers = ransac.tbl$model.inlier / ransac.tbl$total.inliers,
                                 natural = ransac.tbl$model.natural / ransac.tbl$total.natural,
                                 perturbation = ransac.tbl$model.perturbation / ransac.tbl$total.perturbation)

tbl.outlier.melt <- melt(ransac.tbl.outlier, id.vars = c('perct'))
tbl.outlier.melt <- subset(tbl.outlier.melt, !is.nan(value) & !is.nan(value))
#
ggplot(tbl.outlier.melt, aes(x = perct, y = value, color = variable, shape = variable, linetype = variable)) + 
  theme_minimal() + theme(legend.position = "top") + expand_limits(y = c(0,1)) +
  #  facet_wrap( ~ type, ncol = 1) +
  geom_line() + geom_point() + 
    scale_color_manual(name = '', values = c('#0072B2', '#CC79A7')) + 
  scale_shape_manual(name = '', values = c(1, 1, 5, 5)) + 
  scale_linetype_manual(name = '', values = c('solid', 'solid', 'longdash', 'longdash')) +
  #geom_line(data = tbl.melt.ransac) + geom_point(data = tbl.melt.ransac) + 
  labs(title = 'Percentage of data included in the RANSAC Model\n(relative to total inlier/perturbation)',
       subtitle = 'ideal scenario: perturbation = 0 and inliers = 1',
       x = sprintf('Count of perturbed data points (out of %d total observations)', obs),
       y = 'Observations included in model (%)')
```

```{r, eval=FALSE, fig.height=15}
# Plot Everything

print(ggplot(new.ydata) + 
  #
  theme_minimal() + 
  labs(x = '', y = 'Class Probability', shape = "Class", color = "Oulier") + 
  ggtitle('') + 
  # Class distinction
  geom_vline(aes(xintercept = count.0), color = 'black', linetype = 'dotted') +
  geom_text(aes(x=count.a), y = 1.05, label= "Class 0") +
  geom_text(aes(x=count.b), y = 1.05, label= "Class 1") +
  #
  scale_shape_manual(values=c(1,9)) +
  scale_color_manual(values=c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")) +
  scale_y_continuous(minor_breaks = waiver(), breaks = c(0,0.25, 0.5, .75, 1), expand = c(0.05,0.05)) + 
  scale_x_discrete(breaks = NULL) +
  #
  geom_point(aes(x = x, y = prob, color = outliers, shape = logit_class)) +
  facet_wrap( ~ pert, ncol = 5)
)
```



```{r,include=FALSE,eval=FALSE}
new.xdata <- melt(data.frame(id = seq(nrow(xdata.train)), 
                             #xdata.train[,sample(seq(nrow(xdata.train)), 3)]),
                             xdata.train), 
                  id.vars = 'id')
summary(new.xdata$value)
ggplot(new.xdata) + geom_freqpoly(aes(value), binwidth = .1) + facet_wrap( ~ variable)
```

