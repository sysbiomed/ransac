---
title: "RANSAC Noise"
author: "André Veríssimo - IDMEC/IST - SELS"
output: 
  html_document:
    toc: true
    dev: svg
    self_contained: true
    number_sections: true
params:
  mc.cores: !r 12
  # seed to make reproducible results
  my.seed: !r 1991
  
  # Number of lambdas to test
  nlambda: !r 100
  # Minimum ratio of lambda from initial
  lambda.min.ratio: !r 1e-5
  
  # training / test percentage
  train.perc: !r 1

  #
  perturbation.perct: !r seq(0, .5, 0.005)

  #
  # Ransac
 
  # use pre-set models -- only need to change k and my.family
  pre.set: '10vars-sparse'
  
  # number of iterations
  k: !r 500
  
  # type of model to use
  my.family: 'binomial.glm.auc'
  
  # Number of observations in data
  obs: !r 100
  
  # True model
  base.model: !r c(0,2,-3,1.5)
  
  # minimum number of points to use
  n: !r 30
  # threshold to identify inliers
  threshold: !r .04
  # number of inliers to consider good model
  good.fit.perct: !r .1
  # chose one of the lambda / alpha / penalty
  alpha.baseline: !r 0
  
  #
  # Show arguments
  
  only.show.mod: !r .05
---

```{r setup,include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries,include=FALSE}
## Libraries required

# output nothing
suppressPackageStartupMessages({
  library(devtools)
  source(file.path('..', 'R-aux', 'dataset_brca_tnbc.R'))
  #library(ransac)
  devtools::load_all('../')
  library(brca.data)
  library(futile.logger)
  library(glmnet)
  library(verissimo)
  library(dplyr)
  library(ggbeeswarm)
  library(ggplot2)
  library(reshape2)
  library(digest)
})
```

```{r logging.settings,include=FALSE}
# Setup logging to write to file `logger.txt` and using DEBUG level

#flog.layout(layout.simple)
layout <- layout.format('~m')
flog.layout(layout)
flog.appender(appender.tee('logger.txt'))
flog.threshold(INFO)
```

# Parameters

```{r params,include=FALSE}
#if (exists('my.params')) {
#  params <- my.params
#}

# create variables with params names
for (ix.name in names(params)) { 
  assign(ix.name, params[[ix.name]])
}

# generated parameters
family.fun <- ransac.family(my.family)
```

```{r, include=FALSE}
## Define synthetic data steps
if (pre.set == '3vars') {
  # True model
  base.model         <- c(0,2,-3,1.5)
} else if (pre.set == '10vars-sparse') {
  set.seed(my.seed)
  base.model         <- c(0,runif(10, min = -10, max = 10))
  base.model[sample(seq(length(base.model)), 10 - 2)] <- 0 # keep only 2 variables
} else if (pre.set == '1000vars-sparse') {
  set.seed(my.seed)
  base.model         <- c(0,runif(1000, min = -10, max = 10))
  base.model[sample(seq(length(base.model)), 1000 - 80)] <- 0 # keep only 80 variables
}

#
penalty.factor <- array(1, length(base.model) - 1)
sort.by        <- 'logit_class'
ransac.tbl     <- data.frame()
baseline.tbl   <- data.frame()
```


```{r, include=FALSE}
#
#
my.random <- function(length) { 
  rlnorm(length, meanlog = runif(1, min = 0, max= 1), sdlog = runif(1, min = .5, max= 1))
}

#
#
print.outlier.table <- function(my.model, xdata.train, ydata.train, lambda.baseline) {
  
  miscl.all.ix <- ydata.train$outliers %in% c('perturbation', 'natural')
  flog.info('')
  flog.info('Trained on the perturbed dataset and tested against it.')
  miscl.pert.prob <- family.fun$predict(my.model, newx = xdata.train, lambda = lambda.baseline)

  miscl.pert <- as.numeric(levels(ydata.train$logit_class)[ydata.train$logit_class]) - round(miscl.pert.prob)
  flog.info('')
  flog.info('```{r}')
  flog.info('Perturbed model:\n    % 6d false p.\n  + % 6d false n.\n  -----------------\n    % 6d / %d',
            sum(miscl.pert > 0), sum(miscl.pert < 0), sum(miscl.pert != 0), length(miscl.pert))
  flog.info('')
  flog.info('```')
  my.data$ydata$outliers[miscl.pert != 0]
  miscl.pert.diff <- ydata.train$prob - abs(round(miscl.pert.prob))
  flog.info('')
  table.ix <- miscl.pert != 0 | miscl.all.ix
  table.aux <- cbind(id = rownames(ydata.train)[table.ix],
                     ydata.train[table.ix,], 
                     predicted_prob = miscl.pert.prob[table.ix],
                     pred_class = round(miscl.pert.prob[table.ix]),
                     diff_class = miscl.pert[table.ix])

  table.aux <- arrange_(table.aux, 'outliers', 'diff_class', 'id')
  flog.info('')
  flog.info('Notes on significance of predicted, as it is calculated by logit_class(i) - round(pred(i))')
  flog.info('')
  flog.info(' - -1: false positive')
  flog.info(' -  0: correct classification')
  flog.info(' - +1: false negative')
  flog.info('')
  flog.info('Table with all outliers in model (plus all perturbed/natural)')
  flog.info('')
  cat(knitr::kable(table.aux), sep = '\n')
  # not showing the distribution of error
  #flog.info('')
  #flog.info('')
  #print(ggplot(melt(miscl.pert.diff)) + theme_minimal() +
  #  geom_freqpoly(aes(value), color = '#E69F00', binwidth = 0.05) + 
  #  ggtitle('Distribution of error to real probabilities'))
  flog.info('')
  
  return(data.frame(perct = perct,
                    inliers = sum(abs(table.aux$diff_class[table.aux$outliers == 'inlier'])),
                    total.inliers = sum(ydata.train$outliers == 'inlier'),
                    natural = sum(abs(table.aux$diff_class[table.aux$outliers == 'natural'])),
                    total.natural = sum(ydata.train$outliers == 'natural'),
                    perturbation = sum(abs(table.aux$diff_class[table.aux$outliers == 'perturbation'])),
                    total.perturbation = sum(ydata.train$outliers == 'perturbation')))
}
```

```{r show.params,echo=FALSE}
{
  flog.info('Parameters:')
  flog.info('')
  flog.info('---- General --------------------')
  flog.info('                           Family: %s', my.family)
  flog.info('         initial seed for reprod.: %d', my.seed)
  flog.info('                       # of cores: %d', mc.cores)
  flog.info('             # of lambdas to test: %d', nlambda)
  flog.info('                 lambda.min.ratio: %g', lambda.min.ratio)
  flog.info('----------------------------------')
  flog.info('')
  flog.info('---- True Model ------------------')
  flog.info('                    pre.set model: %s', pre.set)
  flog.info('                    vars in model: %d', length(base.model) - 1)
  flog.info('           non-zero vars in model: %d', sum(base.model[2:length(base.model)] != 0))
  flog.info('                        intercept: %g', base.model[1])
  flog.info('----------------------------------')
  flog.info('')
  flog.info('---- RANSAC ----------------------')
  flog.info('                      Train set %%: %g', train.perc)
  flog.info('                     k iterations: %d', k)
  flog.info('          minimum number of cases: %d', n)
  flog.info('      threshold to keep (squared): %g', threshold)
  flog.info('   %% of inliers to consider model: %g', good.fit.perct)
  flog.info('       alpha to be used in RANSAC: %g', alpha.baseline)
}
```

```{r, eval=FALSE, include=FALSE}
alpha.baseline <- .5
#
set.seed(my.seed)
perct <- 0.01
my.data <- gen.synth(obs = obs, 
                     new.coef = base.model, 
                     perturbation.perct = perct, 
                     xdata.fun = my.random,
                     perfect.model = TRUE,
                     perturbation.random = TRUE)
my.data$ydata
#
xdata.train <- my.data$xdata

# normalize xdata
xdata.train.mean <- sapply(seq(ncol(xdata.train)), function(ix) { mean(xdata.train[,ix])})
xdata.train.sd   <- sapply(seq(ncol(xdata.train)), function(ix) { sd(xdata.train[,ix])})
#xdata.train <- xdata.train / xdata.train.sd
xdata.train <- (xdata.train - xdata.train.mean) / xdata.train.sd

ydata.train <- as.data.frame(my.data$ydata)
ydata.train$logit_class <- as.factor(ydata.train$logit_class)
ydata.train$real_class  <- as.factor(ydata.train$real_class)
  
colnames(ydata.train) <- colnames(my.data$ydata)

baseline <- cv.glmnet(x                = xdata.train, 
                      y                = ydata.train$logit_class,
                      family           = 'binomial', 
                      alpha            = alpha.baseline,
                      nlambda          = 100, 
                      lambda.min.ratio = lambda.min.ratio,
                      #
                      standardize    = F,
                      intercept      = F,
                      nfolds         = 10,
                      foldid         = foldid,
                      penalty.factor = penalty.factor,
                      mc.cores       = 1)

  
#
lambda.baseline <- baseline$lambda.min

# perturbed model
suppressWarnings({
  baseline.model.pert <- family.fun$fit.model(my.data$xdata, my.data$ydata$logit_class,
                                              lambda.baseline, penalty.factor = penalty.factor,
                                              alpha          = alpha.baseline)
})

#
baseline.coef <- coef(baseline.model.pert, s = lambda.baseline)[,1]
t(rbind(base.model,baseline.coef))
aa <- data.frame(prediction = family.fun$predict(baseline.model.pert, newx = xdata.train, lambda = lambda.baseline)[,1],
                 observation = as.numeric(levels(ydata.train$logit_class))[ydata.train$logit_class])
aa <- arrange_(aa, 'observation', 'prediction')
aa$id <- seq(nrow(aa))
ggplot(melt(aa, id.vars = 'id')) + theme_minimal() +
  geom_point(aes(y = value, x = id, color = variable)) +
  geom_vline(aes(xintercept = sum(ydata.train$logit_class == 0) + .5), color = 'black', linetype = 'dotted')

#print.outlier.table(baseline.model.pert, xdata.train, ydata.train, lambda.baseline)
```


# Generate data and ransac vs. baseline

```{r,results='asis',echo=FALSE}
#for (my.seed in 1985:2017) {
new.ydata <- data.frame()
for (perct in perturbation.perct) {
  set.seed(my.seed)
  
  my.data <- gen.synth(obs = obs, 
                       new.coef = base.model, 
                       perturbation.perct = perct, 
                       xdata.fun = my.random,
                       perfect.model = TRUE,
                       perturbation.random = TRUE)
  #
  #
  sorted.ix <- sort(my.data$ydata[[sort.by]], index.return = T)$ix
  my.data$xdata <- my.data$xdata[sorted.ix,]
  my.data$ydata <- my.data$ydata[sorted.ix,]
  #
  my.data$ydata$logit_class <- as.factor(my.data$ydata$logit_class)
  my.data$ydata$real_class  <- as.factor(my.data$ydata$real_class)
  #
  class.0.count <- sum(my.data$ydata[[sort.by]] == 0)
  class.1.count <- sum(my.data$ydata[[sort.by]] == 1)
  
  # Build big table
  new.ydata <- rbind(new.ydata, 
                     cbind(my.data$ydata, 
                           pert = perct, 
                           count.0 = class.0.count + .5, 
                           count.a = class.0.count / 2, 
                           count.b = class.0.count * 1.5, 
                           x = seq(nrow(my.data$ydata))))
  #
  #  
  # GLMNET 
  
  # Estimate lambda
  
  sets.ix <- balanced.train.and.test(which(my.data$ydata$logit_class == 1), 
                                     which(my.data$ydata$logit_class == 0), 
                                     train.perc = 1, join.all = T)
  train.ix <- sets.ix$train
  test.ix  <- sets.ix$test
  
  #  
  xdata.train <- my.data$xdata[train.ix, ]
  # normalize xdata
  xdata.train.mean <- sapply(seq(ncol(xdata.train)), function(ix) { mean(xdata.train[,ix])})
  xdata.train.sd   <- sapply(seq(ncol(xdata.train)), function(ix) { sd(xdata.train[,ix])})
  #xdata.train <- xdata.train / xdata.train.sd
  xdata.train <- (xdata.train - xdata.train.mean) / xdata.train.sd
  
  ydata.train <- as.data.frame(my.data$ydata[train.ix,])
  colnames(ydata.train) <- colnames(my.data$ydata)
  
  if (grepl('GLMNET', family.fun$model.name)) {
    cv.folds <- balanced.cv.folds(which(ydata.train$logit_class == 0), which(ydata.train$logit_class == 1))
    foldid   <- array(0, nrow(ydata.train))
    foldid[ydata.train$logit_class == 0] <- cv.folds$output[[1]]
    foldid[ydata.train$logit_class == 1] <- cv.folds$output[[2]]
    #   
    baseline <- cv.glmnet(xdata.train, 
                          as.numeric(levels(ydata.train$logit_class))[ydata.train$logit_class], 
                          family = 'binomial', 
                          alpha            = alpha.baseline,
                          nlambda          = 100, 
                          lambda.min.ratio = lambda.min.ratio,
                          #
                          standardize    = F,
                          intercept      = F,
                          nfolds         = 10,
                          foldid         = foldid,
                          penalty.factor = penalty.factor,
                          mc.cores       = 1)
    lambda.baseline <- baseline$lambda.min
  } else {
    lambda.baseline <- 0
    alpha.baseline <- 0
  }
  #
  
  # Create baseline models
  
  # perturbed model
  suppressWarnings({
  baseline.model.pert <- family.fun$fit.model(xdata.train, ydata.train$logit_class,
                                              lambda.baseline, penalty.factor = penalty.factor,
                                              alpha          = alpha.baseline)
  })
  # Ransac

  ransac.cache <- file.path('..', 
                            'cache', 
                            sprintf('noise-ransac-%.3fk_%d-n_%d-f_%s-t_%.4f-g_%.4f-a_%.3f-p_%s-x_%s-y_%s.RData',
                                    perct,
                                    k, 
                                    n, 
                                    my.family,
                                    threshold,
                                    good.fit.perct,
                                    alpha.baseline,
                                    strtrim(digest(penalty.factor, algo = 'md5'), 10),
                                    strtrim(digest(xdata.train, algo = 'md5'), 10),
                                    strtrim(digest(ydata.train, algo = 'md5'), 10)))
  if (file.exists(ransac.cache)){
    load(ransac.cache)
  } else {
    do.nothing <- capture.output({
      result.ransac <- ransac::ransac(xdata  = xdata.train, 
                                      ydata  = as.numeric(levels(ydata.train$logit_class)[ydata.train$logit_class]), 
                                      k      = k, 
                                      n      = n, 
                                      family = my.family, 
                                      #
                                      threshold      = threshold, 
                                      good.fit.perct = good.fit.perct, 
                                      #
                                      mc.cores       = mc.cores,
                                      lambda         = lambda.baseline,
                                      alpha          = alpha.baseline,
                                      intercept      = F,
                                      penalty.factor = penalty.factor)
    })
    save(result.ransac, lambda.baseline, file = ransac.cache)
  }
  
  #
  #
  baseline.output <- capture.output({
    baseline.tbl.newline <- print.outlier.table(baseline.model.pert, xdata.train, ydata.train, lambda.baseline)
  })
  
  baseline.tbl <- rbind(baseline.tbl, baseline.tbl.newline)
  
  #
  #
  has.ransac.warning <- FALSE
  if (length(result.ransac$models$all.inliers.consensus) < 1 || is.na(result.ransac$models$all.inliers.consensus)) {
    ransac.tbl.newline <- data.frame(perct = perct,
                                   inliers = NA,
                                   total.inliers = sum(ydata.train$outliers == 'inlier'),
                                   natural = NA,
                                   total.natural = sum(ydata.train$outliers == 'natural'),
                                   perturbation = NA,
                                   total.perturbation = sum(ydata.train$outliers == 'perturbation'))
    ransac.tbl <- rbind(ransac.tbl, ransac.tbl.newline)
    has.ransac.warning <- TRUE
  } else {
    ransac.output <- capture.output({
      ransac.tbl.newline <- print.outlier.table(result.ransac$models$all.inliers.consensus, xdata.train, ydata.train, lambda.baseline)
    })
    ransac.tbl <- rbind(ransac.tbl,ransac.tbl.newline)
  }

  #
  #
  # Output
  # 
  
  if (only.show.mod == FALSE || perct %% only.show.mod == 0) {
    flog.info('\n## Testing for noise %% at %.3f', perct)
    #
    flog.info('\n### Data')
    flog.info('')
    flog.info('    Counts:')
    flog.info('      Class 0: %d', class.0.count)
    flog.info('      Class 1: %d', class.1.count)
    #
    flog.info('')
    
    print(ggplot(my.data$ydata) + 
      #
      theme_minimal() + 
      labs(x = '', y = 'Class Probability', shape = "Class", color = "Oulier") + 
      ggtitle('') + 
      # Class distinction
      geom_vline(aes(xintercept = class.0.count + .5), color = 'black', linetype = 'dotted') +
      geom_text(aes(x=class.0.count / 2), y = 1.05, label= "Class 0") +
      geom_text(aes(x=class.0.count + class.1.count / 2), y = 1.05, label= "Class 1") +
      #
      scale_shape_manual(values=c(1,9)) +
      scale_color_manual(values=c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")) +
      scale_y_continuous(minor_breaks = waiver(), breaks = c(0,0.25, 0.5, .75, 1), expand = c(0.05,0.05)) + 
      scale_x_discrete(breaks = NULL) +
      #
      geom_point(aes(x = seq(nrow(my.data$ydata)), y = prob, color = outliers, shape = logit_class)))
    #
    flog.info('')
    # 
    
  
    if (grepl('GLMNET', family.fun$model.name)) {
      flog.info('')
      flog.info('    Running GLMNET with cross-validation to find lambda parameter')
      flog.info('      - Found lambda = %g to use in RANSAC if necessary', lambda.baseline)
    }
  
    flog.info('')
    flog.info('')
    flog.info('### RANSAC & Baseline')
    flog.info('')

    suppressWarnings({
      plot.info <- plot(result.ransac, xdata = xdata.train, 
           ydata          = as.numeric(levels(ydata.train$real_class)[ydata.train$real_class]),
           baseline       = baseline.model.pert,
           family         = my.family, 
           name           = sprintf('Perct: %.3f (distance to class)', perct),
           lambda         = lambda.baseline,
           alpha          = alpha,
           only_consensus = T,
           show.misclass = F)
    })
  
    flog.info('\n```{r}')
    cat( plot.info$debug, sep = '\n')
    flog.info('```')
    
    #
    #
    flog.info('')
    flog.info('### Misclassification Tables')
    flog.info('')
    flog.info('#### Baseline')
  
    cat(baseline.output, sep = '\n')
  
    flog.info('')
    flog.info('#### RANSAC Consensus')
  
    if (has.ransac.warning) {
      flog.info('')
      flog.info('```{r}')
      flog.info('WARNING: RANSAC consensus does not have any good fit with %.3f of perturbation', perct)
      flog.info('```')
    } else {
      cat(ransac.output, sep = '\n')
      #my.model <- result.ransac$models$all.inliers.consensus
      #result.ransac$error.array[[1000]]$inliers.ix
    }  
  
    
  }
}
#} for to test seed
```

# Summary of noise experiments

```{r, echo=FALSE}
baseline.tbl.sum <- data.frame(perct = baseline.tbl$perct,
                               inliers = baseline.tbl$inliers / baseline.tbl$total.inliers,
                               natural = baseline.tbl$natural / baseline.tbl$total.natural,
                               perturbation = baseline.tbl$perturbation / baseline.tbl$total.perturbation,
                               type = array('Baseline', nrow(baseline.tbl)))

# 
ransac.tbl.sum <- data.frame(perct = ransac.tbl$perct,
                               inliers = ransac.tbl$inliers / ransac.tbl$total.inliers,
                               natural = ransac.tbl$natural / ransac.tbl$total.natural,
                               perturbation = ransac.tbl$perturbation / ransac.tbl$total.perturbation,
                               type = array('RANSAC Consensus', nrow(ransac.tbl)))

tbl.melt <- melt(rbind(baseline.tbl.sum, ransac.tbl.sum), id.vars = c('perct', 'type'))
tbl.melt$identifier <- paste0(tbl.melt$type, ' ', tbl.melt$variable)
tbl.melt <- subset(tbl.melt, !is.nan(value))
#
ggplot(tbl.melt, aes(x = perct, y = value, color = identifier, shape = identifier, linetype = identifier)) + 
  theme_minimal() + theme(legend.position = "bottom") + expand_limits(y = c(0,1)) +
  #  facet_wrap( ~ type, ncol = 1) +
  geom_line() + geom_point() + 
    scale_color_manual(name = 'Combined', values = c('#9E5D00', '#007700', '#D55E00', '#009E73')) + 
  scale_shape_manual(name = 'Combined', values = c(1, 1, 5, 5)) + 
  scale_linetype_manual(name = 'Combined', values = c('solid', 'solid', 'longdash', 'longdash')) +
  #geom_line(data = tbl.melt.ransac) + geom_point(data = tbl.melt.ransac) + 
  ggtitle('Baseline model: Misclassification rate by type of data point')
```


# Plot Everything

```{r, include=TRUE, fig.height=15}
print(ggplot(new.ydata) + 
  #
  theme_minimal() + 
  labs(x = '', y = 'Class Probability', shape = "Class", color = "Oulier") + 
  ggtitle('') + 
  # Class distinction
  geom_vline(aes(xintercept = count.0), color = 'black', linetype = 'dotted') +
  geom_text(aes(x=count.a), y = 1.05, label= "Class 0") +
  geom_text(aes(x=count.b), y = 1.05, label= "Class 1") +
  #
  scale_shape_manual(values=c(1,9)) +
  scale_color_manual(values=c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")) +
  scale_y_continuous(minor_breaks = waiver(), breaks = c(0,0.25, 0.5, .75, 1), expand = c(0.05,0.05)) + 
  scale_x_discrete(breaks = NULL) +
  #
  geom_point(aes(x = x, y = prob, color = outliers, shape = logit_class)) +
  facet_wrap( ~ pert, ncol = 5)
)
```



```{r,eval=FALSE,include=FALSE}
new.xdata <- melt(data.frame(id = seq(nrow(xdata.train)), 
                             #xdata.train[,sample(seq(nrow(xdata.train)), 3)]),
                             xdata.train), 
                  id.vars = 'id')
summary(new.xdata$value)
ggplot(new.xdata) + geom_freqpoly(aes(value), binwidth = .1) + facet_wrap( ~ variable)
```

